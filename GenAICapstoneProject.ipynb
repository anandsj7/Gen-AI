{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35QlEf8ENFKA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Step 1: Load CSV File\n",
        "csv_file_path = \"/content/Conversation.csv\"  # Replace with your actual file path\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Check the CSV structure to ensure 'question' and 'answer' columns exist\n",
        "print(df.head())\n",
        "\n",
        "# Step 2: Prepare the Dataset (Ensure CSV has 'question' and 'answer' columns)\n",
        "questions = df['question'].tolist()\n",
        "answers = df['answer'].tolist()\n",
        "\n",
        "# Prepare training data for fine-tuning\n",
        "training_data = [{\"question\": q, \"answer\": a} for q, a in zip(questions, answers)]\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "dataset = Dataset.from_dict({\"question\": questions, \"answer\": answers})\n",
        "\n",
        "# Step 3: Initialize GPT-2 Tokenizer and Model\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set padding and EOS token handling\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Step 4: Tokenize the Dataset (concatenate question and answer)\n",
        "def tokenize_function(examples):\n",
        "    # Concatenate question and answer for training\n",
        "    questions = examples['question']\n",
        "    answers = examples['answer']\n",
        "\n",
        "    # Concatenate questions and answers for each example\n",
        "    prompts = [q + \" \" + tokenizer.eos_token + \" \" + a for q, a in zip(questions, answers)]\n",
        "\n",
        "    # Tokenize the list of concatenated strings\n",
        "    return tokenizer(prompts, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Apply tokenization to the dataset\n",
        "train_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Step 5: Fine-Tune the Model\n",
        "# Set training arguments for fine-tuning\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Step 6: Generate Response with the Fine-Tuned Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def generate_response(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    output = model.generate(input_ids, max_length=150, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, no_repeat_ngram_size=2, top_k=50)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test the fine-tuned chatbot\n",
        "question = \"What is AI?\"\n",
        "response = generate_response(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Response: {response}\")\n"
      ]
    }
  ]
}